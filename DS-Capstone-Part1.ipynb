{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting severe occurrences of automobile collisions in Seattle, WA\n",
    "\n",
    "# ```Part 1: Pre-Processing```"
   ]
  },
  {
   "source": [
    "## Importing Libraries "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import missingno as msno\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "*** \n",
    "## Download and Load collision dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### This dataset was downloaded on 22 September, 2020\n",
    "!wget -O Collisions.csv 'http://data-seattlecitygis.opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv?outSR={%22latestWkid%22:2926,%22wkid%22:2926}'"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import collision dataset and display the first five rows\n",
    "df = pd.read_csv(\"Collisions_Seattle.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "source": [
    "### Review the characteristics of the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print()\n",
    "print(df.dtypes)"
   ]
  },
  {
   "source": [
    "The dataset has 221,525 rows and 40 columns. This identifies multiple variables with a significant amount of missing data. For example: \n",
    "\n",
    "```INTKEY``` (71,936); ```EXCEPTRSNCODE``` (101,122); ```EXCEPTRSNDESC``` (11,779); ```INATTENTIONIND``` (30,188); ```PEDROWNOTGRNT``` (5,192); ```SPEEDING``` (9,929)\n",
    "\n",
    "A large number of missing values could cause noise and bias in the results, therefore will need to be carefully evaluated. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "source": [
    "This is a summary statisics of the variables - excluding those that are catagorical. \n",
    "\n",
    "This has given us a quick overview of the contents of the collision dataset.  \n",
    "At this stage it would be benefitial to get a profile of each of the variables.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "## Determine which variables to exclude\n",
    "### OBJECTID | INCKEY | COLDETKEY | LOCATION\n",
    "\n",
    "```OBJECTID``` is an unique has a unique value for each incident, which corresponds with the dataframe index value, therefore, ```OBJECTID``` can be dropped.\n",
    "\n",
    "```INCKEY```, ```COLDETKEY```, and REPORTNO each have unique values for each collision incident, therefore, they can be dropped.\n",
    "\n",
    "```INTKEY``` has too many missing entries to be used.\n",
    "\n",
    "```LOCATION``` is a catagorical field that can be substitued by latitude and longitude variables (```X``` and ```Y``` variables respectively)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### EXCEPTRSNCODE | EXCEPTRSNDESC\n",
    "\n",
    "```EXCEPTRSNCODE``` and ```EXCEPTRSNDESC``` will be dropped since they have too many missing values to be used in a model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (df['EXCEPTRSNCODE'].value_counts())\n",
    "print ()\n",
    "print (df['EXCEPTRSNDESC'].value_counts())"
   ]
  },
  {
   "source": [
    "### SEVERITYCODE | SEVERITYDESC\n",
    "\n",
    "```SEVERITYCODE``` and ```SEVERITYDESC``` are duplicate information. \n",
    "\n",
    "```SEVERITYCODE``` will be dropped since ```SEVERITYDESC``` has more information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the frequency of the values for 'INATTENTIONIND'\n",
    "severity = pd.DataFrame()\n",
    "severity['severity'] = df['SEVERITYDESC'].value_counts()\n",
    "severity['percent'] = (severity['severity']/sum(severity['severity'])*100)\n",
    "print (\"Frequency of severity categories\")\n",
    "print ()\n",
    "print (severity)"
   ]
  },
  {
   "source": [
    "### INCDATE | INCDTTM\n",
    "\n",
    "```INCDATE``` and ```INCDTTM``` both provide the date of the incident.  Only ```INCDTTM``` is required since it includes both the time and date.  \n",
    "\n",
    "To conduct time series analysis, ```INCDTTM``` needs to be converted from a _string format to the pandas data-time format_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['INCDTTM'] = df['INCDTTM'].apply(pd.to_datetime)\n",
    "print (df['INCDTTM'][:10])"
   ]
  },
  {
   "source": [
    "### INATTENTIONIND | PEDROWNOTGRNT\n",
    "\n",
    "```INATTENTIONIND``` has a significant amount of missing data with only 30,188 values, which all have a value of 'Y' and pertian to the driver not paying attention while driving, and should be dropped.\n",
    "\n",
    "```PEDROWNOTGRNT``` also has a significant amount of missing data with only 5,195 values, which all have a value of 'Y' and pertian to pedestrian right of way was not granted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Determine the frequency of each value for 'INATTENTIONIND' and 'PEDROWNOTGRNT'\n",
    "print (\"Frequency of INATTENTIONIND values:\", df['INATTENTIONIND'].value_counts())\n",
    "print()\n",
    "print (\"Frequency of PEDROWNOTGRNT values: \", df['PEDROWNOTGRNT'].value_counts())"
   ]
  },
  {
   "source": [
    "### SDOT_COLCODE | SDOT_COLDESC\n",
    "\n",
    "```SDOT_COLCODE``` and ```SDOT_COLDESC``` correspond to the same information.   ```SDOT_COLCODE``` is the type of collision, and ```SDOT_COLDESC``` has the description of each type of collision. Therefore, we can drop ```SDOT_COLCODE``` since ```SDOT_COLDESC``` has more information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Unique collision type for SDOT:        \",len(df['SDOT_COLCODE'].unique()))\n",
    "print (\"Unique collision description for SDOT: \",len(df['SDOT_COLDESC'].unique()))"
   ]
  },
  {
   "source": [
    "### ST_COLCODE | ST_COLDESC\n",
    "\n",
    "```ST_COLCODE``` is a code for a description of a collision.\n",
    "\n",
    "```ST_COLDESC``` is the description of a collision based on the states collision coding dictionary.\n",
    "\n",
    "Since these variables are duplicate information it was expected to have the same or similar unique values.  This was not the case.  The ```ST_COLCODE``` needed to be converted from a string to a numeric value.  Rerun of unique values then confirm that ```ST_COLCODE``` and ```ST_COLDESC``` have the same number of unique values.  Therefore, we can drop ```ST_COLCODE``` since ```ST_COLDESC``` has more information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Unique collision type for SDOT:        \",len(df['ST_COLCODE'].unique()))\n",
    "print (\"Unique collision description for SDOT: \",len(df['ST_COLDESC'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ST_COLCODE'] = pd.to_numeric(df['ST_COLCODE'], errors='coerce')\n",
    "print(df.head())\n",
    "print (df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Unique collision type for SDOT:        \",len(df['ST_COLCODE'].unique()))\n",
    "print (\"Unique collision description for SDOT: \",len(df['ST_COLDESC'].unique()))"
   ]
  },
  {
   "source": [
    "### SPEEDING\n",
    "\n",
    "```SPEEDING``` has a significant amount of missing data with only 9929 values, which all have a value of 'Y' and pertian to whether or not speeding was a factor in the collision."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Determine the frequency of each value for 'SPEEDING'\n",
    "print (\"Frequency of SPEEDING values:\", df['SPEEDING'].value_counts())\n"
   ]
  },
  {
   "source": [
    "### SEGLANEKEY\n",
    "\n",
    "```SEGLANEKEY``` contains 2101 unique values with the value '0' dominating with 218,489 of the observations. The distribution of unique values is highly skewed indicating that ```SEGLANEKEY``` should be dropped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Unique values for SEGLANEKEY: \",len(df['SEGLANEKEY'].unique()))\n",
    "print ()\n",
    "print (df['SEGLANEKEY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of SEGLANEKEY\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(data=df, x='SEGLANEKEY', bins=50)\n",
    "plt.title(\"Distribution of SEGLANEKEY\")\n",
    "plt.xlabel(\"Histogram with fixed size bins (bins=50)\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### CROSSWALKKEY\n",
    "\n",
    "```CROSSWALKKEY``` contains 2343 unique values with the value '0' dominating with 217,283 of the observations. The distribution of unique values is highly skewed indicating that ```CROSSWALKKEY``` should be dropped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Unique values for CROSSWALKKEY: \",len(df['CROSSWALKKEY'].unique()))\n",
    "print ()\n",
    "print (df['CROSSWALKKEY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of SEGLANEKEY\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(data=df, x='CROSSWALKKEY', bins=50)\n",
    "plt.title(\"Distribution of CROSSWALKKEY\")\n",
    "plt.xlabel(\"Histogram with fixed size bins (bins=50)\")"
   ]
  },
  {
   "source": [
    "### Remove Fields\n",
    "\n",
    "Remove the fields (columns) that have been identified as not required."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop([\"OBJECTID\", \"STATUS\", \"INCKEY\",\"COLDETKEY\",\"REPORTNO\",\"INTKEY\",\"LOCATION\", \"EXCEPTRSNCODE\",\"EXCEPTRSNDESC\",\"SEVERITYCODE\", \"INCDATE\",\"INATTENTIONIND\",\"PEDROWNOTGRNT\",\"SPEEDING\", \"SDOTCOLNUM\",\"SDOT_COLCODE\", \"ST_COLCODE\", \"SEGLANEKEY\", \"CROSSWALKKEY\"],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "***\n",
    "## Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Identify variables with missing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "source": [
    "### Missing Values Matrix & Heat Map\n",
    "\n",
    "Using the missingno library to plot and identify where the missing values are located in each column and correlations between missing values across different columns.\n",
    "\n",
    "The white lines indicate missing values.  Most of the columns with missing values occur in common rows.  Note the strong correlation between COLLISIONTYPE, UNDERINF, WEATHER, ROADCOND, LIGHTCOND, and ST_COLDESC.\n",
    "\n",
    "The percentage cutoff for this plot shows variables that are at most 99.9% complete.  Any remaining variables with missing data will be considered negligable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = msno.nullity_filter(df, filter='bottom', p=0.999)\n",
    "sorted_data = msno.nullity_sort(filtered_data, sort='descending')\n",
    "msno.matrix(sorted_data.sample(221525))"
   ]
  },
  {
   "source": [
    "### Missing Values Heat Map\n",
    "\n",
    "The heat map is another view of the correlation of variables with missing data.  The strength of the correlation is shown with values in the heat map."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(df,filter='bottom', p=0.999)"
   ]
  },
  {
   "source": [
    "The heat map confirms the correlation of the missing data for the same rows for ```COLLISIONTYPE```, ```UNDERINFL```, ```WEATHER```, ```ROADCOND```, ```LIGHTCOND```, and ```ST_COLDESC```.  Variable ```JUNCTIONTYPE``` has about a 30% correlation with these variables.\n",
    "\n",
    "Another strong positive corrlation is with the ```X``` and ```Y``` variables (Latitude and Longitude respectively).  ```ADDRTYPE``` correlates with ```X``` and ```Y``` for about 70% of the rows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Drop Missing Values for Location Coordinates\n",
    "\n",
    "Removing rows of data corresponding with missing data for ```X``` and ```Y``` should remove all missing data found in ```ADDRTYPE```.  This is verified with df.info() after removing the null values of ```X``` and ```Y```.\n",
    "\n",
    "After dropping these null values there remains 214,050 rows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***\n",
    "## Investigate variables through visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop null values in X and Y\n",
    "df = df.dropna(subset=['X','Y'])\n",
    "\n",
    "# Verify all null values for ADDRTYPE are removed\n",
    "df[['X','Y','ADDRTYPE']].info()"
   ]
  },
  {
   "source": [
    "### Address Type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "splot = sns.countplot(data=df, x = 'ADDRTYPE', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### Severity Description"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "splot = sns.countplot(data=df, x = 'SEVERITYDESC', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### Collision Type\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "splot = sns.countplot(data=df, x = 'COLLISIONTYPE', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### Junction Type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xticks(rotation=90,size=12)\n",
    "splot = sns.countplot(data=df, x = 'JUNCTIONTYPE', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "###  Description of Collision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,24))\n",
    "splot = sns.countplot(data=df, y = 'SDOT_COLDESC', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_width(),), (p.get_width(), p.get_y() + p.get_height()/2.), ha = 'center', va = 'center', xytext = (15, 0), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### Under the Influence\n",
    "\n",
    "This is a binary variable that has two different binary designations. First is 'N' or 'Y'.  The second is '0' or '1'.  The '0' corresponds to 'N' and '1' is 'Y'.  Therefore, replace all '0' values with 'N' and '1' with 'Y'. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (df['UNDERINFL'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UNDERINFL'] = df['UNDERINFL'].replace({'N':0,'Y':1,'0':0,'1':1})\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "splot = sns.countplot(data=df, x = 'UNDERINFL', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### Weather"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xticks(rotation=90,size=12)\n",
    "splot = sns.countplot(data=df, x = 'WEATHER', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### Road Condition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "splot = sns.countplot(data=df, x = 'ROADCOND', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### Light Condition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xticks(rotation=90,size=12)\n",
    "splot = sns.countplot(data=df, x = 'LIGHTCOND', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), ), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "### State Collision Description"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,24))\n",
    "splot = sns.countplot(data=df, y = 'ST_COLDESC', palette='ocean_d', alpha=0.75)\n",
    "\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_width(),), (p.get_width(), p.get_y() + p.get_height()/2.), ha = 'center', va = 'center', xytext = (15, 0), textcoords = 'offset points')"
   ]
  },
  {
   "source": [
    "***\n",
    "## Edit some variables values and formats\n",
    "### Unknowns\n",
    "At this stage all missing values are resolved except for those that fall into one of the variables catagory.  \n",
    "These will be designated as the \"Unknown\" for each variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[:,['COLLISIONTYPE','SDOT_COLDESC', 'JUNCTIONTYPE', 'UNDERINFL','WEATHER','ROADCOND','LIGHTCOND','ST_COLDESC']] = df.loc[:,['COLLISIONTYPE','SDOT_COLDESC', 'JUNCTIONTYPE', 'UNDERINFL','WEATHER','ROADCOND','LIGHTCOND','ST_COLDESC']].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (df['COLLISIONTYPE'].value_counts())\n",
    "print ()\n",
    "print (df['SDOT_COLDESC'].value_counts())\n",
    "print ()\n",
    "print (df['JUNCTIONTYPE'].value_counts())\n",
    "print ()\n",
    "print (df['UNDERINFL'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (df['WEATHER'].value_counts())\n",
    "print ()\n",
    "print (df['ROADCOND'].value_counts())\n",
    "print ()\n",
    "print (df['LIGHTCOND'].value_counts())\n",
    "print ()\n",
    "print (df['ST_COLDESC'].value_counts())"
   ]
  },
  {
   "source": [
    "### Date and Time\n",
    "There will be many factors to analyze with regards to the date and time of events.  Therefore, the ```INCDTTM``` timestamp will be parsed into new fields."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['INCDTTM'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "from datetime import date\n",
    "# import calendar\n",
    "\n",
    "df['year'] = df['INCDTTM'].apply(lambda x: x.year)\n",
    "df['month'] = df['INCDTTM'].apply(lambda x: x.month)\n",
    "df['day'] = df['INCDTTM'].apply(lambda x: x.day)\n",
    "df['hour'] = df['INCDTTM'].apply(lambda x: x.hour)\n",
    "df['minute'] = df['INCDTTM'].apply(lambda x: x.minute)\n",
    "df['weekday'] = df['INCDTTM'].apply(lambda x: x.weekday())  # iso weekday (starts with Monday = 0)\n",
    "# df['day_of_week'] = df['INCDTTM'].dt.day_name() # string name of weekday\n",
    "\n",
    "df[['year','month','day','hour','minute','weekday']].head()"
   ]
  },
  {
   "source": [
    "Approximately a forth of the time stamp (hours, minutes, seconds) have value of '0'.  Unless midnight is truly a bewitching hour these values should not be included when analyzing incidents by hour."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = df[(df.hour > 0) & (df.minute > 0)]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.xticks(rotation=90,size=12)\n",
    "splot = sns.countplot(data=temp, x = 'hour', palette='ocean_d', alpha=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(range(len(df['INJURIES'])),df['INJURIES'])\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(range(len(df['SERIOUSINJURIES'])),df['SERIOUSINJURIES'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[(df.INJURIES < 50) & (df.SERIOUSINJURIES < 30)]\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(range(len(df['INJURIES'])),df['INJURIES'])\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(range(len(df['SERIOUSINJURIES'])),df['SERIOUSINJURIES'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print()\n",
    "print(df.head())"
   ]
  },
  {
   "source": [
    "### Simplify the number of catagories per variable\n",
    "\n",
    "#### Weather"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WEATHER'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine similar catgories \n",
    "df['WEATHER'] = df['WEATHER'].replace('Other','Unknown')\n",
    "\n",
    "# combine clear and partly cloudy conditions into single catagory\n",
    "df['WEATHER'] = df['WEATHER'].replace('Clear','Clear or Partly Cloudy')\n",
    "df['WEATHER'] = df['WEATHER'].replace('Partly Cloudy','Clear or Partly Cloudy')\n",
    "\n",
    "# Combine severe weather conditions into single catagory\n",
    "df['WEATHER'] = df['WEATHER'].replace('Fog/Smog/Smoke','Severe Conditions')\n",
    "df['WEATHER'] = df['WEATHER'].replace('Sleet/Hail/Freezing Rain','Severe Conditions')\n",
    "df['WEATHER'] = df['WEATHER'].replace('Blowing Sand/Dirt','Severe Conditions')\n",
    "df['WEATHER'] = df['WEATHER'].replace('Blowing Snow','Severe Conditions')\n",
    "df['WEATHER'] = df['WEATHER'].replace('Severe Crosswind','Severe Conditions')\n",
    "\n",
    "df['WEATHER'].value_counts()"
   ]
  },
  {
   "source": [
    "#### Road condition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine similar catgories \n",
    "df['ROADCOND'] = df['ROADCOND'].replace('Other','Unknown')\n",
    "df['ROADCOND'] = df['ROADCOND'].replace(['Snow/Slush','Ice'],'Snow/Ice')\n",
    "df['ROADCOND'] = df['ROADCOND'].replace(['Standing Water','Oil'],'Wet')\n",
    "df['ROADCOND'] = df['ROADCOND'].replace(['Sand/Mud/Dirt'],'Dry')\n",
    "df['ROADCOND'].value_counts()"
   ]
  },
  {
   "source": [
    "#### Light condition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine similar catgories\n",
    "df['LIGHTCOND'] = df['LIGHTCOND'].replace('Other','Unknown')\n",
    "df['LIGHTCOND'] = df['LIGHTCOND'].replace(['Dark - No Street Lights','Dark - Street Lights Off'],'Dark - No Street Lights')\n",
    "df['LIGHTCOND'] = df['LIGHTCOND'].replace(['Dusk','Dawn'],'Dusk/Dawn')\n",
    "df['LIGHTCOND'].value_counts()"
   ]
  },
  {
   "source": [
    "#### Severity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine similar catgories\n",
    "df['SEVERITYDESC'] = df['SEVERITYDESC'].replace(['Fatality Collision','Serious Injury Collision'],'Severe')\n",
    "df['SEVERITYDESC'] = df['SEVERITYDESC'].replace(['Property Damage Only Collision','Injury Collision', 'Unknown'],'Not Severe')\n",
    "df['SEVERITYDESC'].value_counts()"
   ]
  },
  {
   "source": [
    "## Output Pre-Processing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Collisions_clean.csv')"
   ]
  },
  {
   "source": [
    "***\n",
    "***\n",
    "### End of Pre-Processing\n",
    "***\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('mypy36': conda)",
   "display_name": "Python 3.6.12 64-bit ('mypy36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "10533f5185414e52aff6599148c3acae351da694a68c87d837c0845d67911927"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}